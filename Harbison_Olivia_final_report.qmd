---
title: "Music Popularity Prediction Model -- Final Report"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Olivia Harbison"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

```{r packages}
#| echo: false
library(tidyverse)
library(tidymodels)
library(here)

tidymodels_prefer()
set.seed(80)

# note: all data will be loaded throughout the document / as needed
```

## Introduction

The music industry is a fickle beast. Everyone wants to have the next hit and be the next Taylor Swift, but how do we make a star? If we could predict what makes songs or albums popular, we could become the next big thing. This is one of many reasons I chose to explore music popularity for my project.

Although I personally have no interest in becoming a pop star, I love listening to music and I've always been curious as to what makes some songs popular and others not. To dig into this, I found a dataset that contains information scraped from Spotify about individual songs and their popularity[^1]. I will use this data to create a model that predicts how popular music albums are on a scale of 1 to 100.

[^1]: This dataset can be found [here](https://www.kaggle.com/datasets/elemento/music-albums-popularity-prediction?select=train.csv) on kaggle.

## Data Overview

The full dataset used here is 160,000 albums from 1917 to 2021, with the marjority having been released in the last 40 years. For computational efficiency, I'm going to use only albums released between 1980 and 2010. Furthermore, I removed all albums containing fewer than three songs, because I feel they should not be classified as albums. Once these changes were made, the dataset was left with 19,184 observations.

The slimmed down dataset had 39 variables, of which 32 were numerical, 6 were categorical, and 1 was the release date. There was no missingness present, but there were some imbalanced variables that were dealt with in the advanced recipe below. An exploratory data analysis was conducted to explore variable imbalance and relationships between variables. More information can be found below in the appendix.

The target variable in this set of regression models is the popularity of the album. This was scored on a scale of 1 to 100, with 100 being very popular and 1 being not popular. The variable did not need to be transformed. Furthermore, possible relationships between the target variable and predictor variables were explored but no clear relationships were found. There was a slight relationship between popularity and energy level, but not major enough to make any adjustments.

## Methods

For this project, I'm doing a series of regression models to identify the one that performs the best at predicting the numerical target variable (popularity). I'm testing 6 different types of models on two different recipes (for 12 total models), then selecting the best one based on the root-mean-square deviation (RMSE). I chose RMSE as my main assessment metric because it does a good job punishing outliers more than other common metrics. I believe this will be valuable due to the amount of outliers in my dataset. The six models include two baseline models: null and linear, and four more advanced models: K nearest neighbors, random forest, boosted tree, and elastic net. Each of these models will be tuned appropriately (more details below).

To start, I split the data into training and testing groups with an 80/20 proportion. I chose this split because it's a fairly large dataset, so 20% in the testing set should be enough to get a strong assessment while 80% in the training set will be enough to create a good fit for the models. Next, I used v-fold cross validation. I chose this resampling technique instead of another (such as bootstrapping) because although it often has high variance, it is usually more accurate in its best estimations. Due to the high computational complexity of this project, I chose to do only 5 folds and 3 repeats. Furthermore, I stratified by the target variable for increased accuracy.

As mentioned above, two recipes were used with the 6 model types, for a total of 12 models generated. The first recipe, which I will refer to as the *kitchen sink* recipe, includes every predictor variable and only the absolutely necessary feature engineering (ex. creating dummy variables for the categorical variables). This recipe was meant to provide a comparison for my more advanced recipe to see what impact my feature engineering has. This recipe was run on all 6 model types.

The second recipe, which I will call the *advanced* recipe, contains every predictor variable and more advanced feature engineering. Here again I created dummy variables for the categorical variables. Additionally, I included a step to transform the numerical variables that were not naturally normal. I chose this step (step_BoxCox) because it runs through all of the possible exponents for transformation and selects the best one for each variable. This maximized the affect of the transformations. Additionally, for the parametric models, I included interaction terms between variables that I found to be highly correlated during my EDA. Next, I removed variables that had near zero variance or zero variance so they did not disrupt the models' results. Finally, I centered and scaled all of the predictors. 


## Model Building & Selection Results

When building the models used in this project, I tuned all appropriate parameters based on balancing the datasets unique needs and my computer's computational power. The parameters tuned were mtry, min_n and learn_rate for the boosted tree model. For the random forest model mtry and min_n were tuned. For the elastic net model the parameters tuned were penalty and mixture. Finally, for the k nearest neighbor model the neighbors parameter was tuned. For details of how each of the parameters were tuned, please see the appendix.



- results table
      Should reiterate the metric that will be used to compare models and determine which will be
      the final/winning model. Include a table of the best performing model results.  This would 
      be a good section to describe what the best parameters were for each model type.
      Could include a discussion comparing any systematic differences in performance between 
      model types or recipes. 
- final model selection
      which one won, was it surprising


## Final Model Analysis

This is where you fit your final/winning model to the testing data. Assess the final model’s performance with at least the metric used to determine the winning model, but it is also advisable to use other performance metrics (especially ones that might be easier to communicate/understand). Should include an exploration of predictions vs the true values (graph) or a confusion matrix (table). Remember to consider the scale of your outcome variable at this time — did you transform the target variable? If a transformation was used, then you should consider conducting analyses on both the original and transformed scale of the target variable. Is the model any good? It might be the best of the models you tried, but does the effort of building a predictive model really pay off — is it that much better than a baseline/null model? Were there any features of the model you selected that make it the best (e.g. fits nonlinearity well)?

## Conclusion

State any conclusions or discoveries/insights. This is a great place for future work, new research questions, and next steps.

## References

Agarwal, M., & Elemento. (2022). Music Albums Popularity Prediction. Kaggle. <https://www.kaggle.com/datasets/elemento/music-albums-popularity-prediction>

### Appendix: Tuning Parameters

A place to share complex and important technical steps that may highly impact explorations, but the details are too technical to share in main body of the report.

### Appendix: EDA

A place to place a more thorough EDA, if needed. This should not include any data from the testing dataset!

-   put some of the skewed histograms in here
-   put something demonstrating the relationships put into the interaction terms

### Appendix: extras — if needed

Add as many appendices as needed.
