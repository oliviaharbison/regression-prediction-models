---
title: "Music Popularity Prediction Model -- Final Report"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Olivia Harbison"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

```{r packages}
#| echo: false
library(tidyverse)
library(tidymodels)
library(here)
library(patchwork)

tidymodels_prefer()
set.seed(80)

# note: all data will be loaded throughout the document / as needed
```

## Introduction

The music industry is a fickle beast. Everyone wants to have the next hit and be the next Taylor Swift, but how do we make a star? If we could predict what makes songs or albums popular, we could become the next big thing. This is one of many reasons I chose to explore music popularity for my project.

Although I personally have no interest in becoming a pop star, I love listening to music and I've always been curious as to what makes some songs popular and others not. To dig into this, I found a dataset that contains information scraped from Spotify about individual songs and their popularity[^1]. I will use this data to create a model that predicts how popular music albums are on a scale of 1 to 100.

[^1]: This dataset can be found [here](https://www.kaggle.com/datasets/elemento/music-albums-popularity-prediction?select=train.csv) on kaggle.

## Data Overview

The full dataset used here is 160,000 albums from 1917 to 2021, with the marjority having been released in the last 40 years. For computational efficiency, I'm going to use only albums released between 1980 and 2010. Furthermore, I removed all albums containing fewer than three songs, because I feel they should not be classified as albums. Once these changes were made, the dataset was left with 19,184 observations.

The slimmed down dataset had 39 variables, of which 32 were numerical, 6 were categorical, and 1 was the release date. There was no missingness present, but there were some imbalanced variables that were dealt with in the advanced recipe below. An exploratory data analysis was conducted to explore variable imbalance and relationships between variables.

The target variable in this set of regression models is the popularity of the album. This was scored on a scale of 1 to 100, with 100 being very popular and 1 being not popular. The variable did not need to be transformed. Furthermore, possible relationships between the target variable and predictor variables were explored but no clear relationships were found. There was a slight relationship between popularity and energy level, but not major enough to make any adjustments.

## Methods

For this project, I'm doing a series of regression models to identify the one that performs the best at predicting the numerical target variable (popularity). I'm testing 6 different types of models on two different recipes (for 12 total models), then selecting the best one based on the root-mean-square deviation (RMSE). I chose RMSE as my main assessment metric because it does a good job punishing outliers more than other common metrics. I believe this will be valuable due to the amount of outliers in my dataset. The six models include two baseline models: null and linear, and four more advanced models: K nearest neighbors, random forest, boosted tree, and elastic net. Each of these models will be tuned appropriately (more details below).

To start, I split the data into training and testing groups with an 80/20 proportion. I chose this split because it's a fairly large dataset, so 20% in the testing set should be enough to get a strong assessment while 80% in the training set will be enough to create a good fit for the models. Next, I used v-fold cross validation. I chose this resampling technique instead of another (such as bootstrapping) because although it often has high variance, it is usually more accurate in its best estimations. Due to the high computational complexity of this project, I chose to do only 5 folds and 3 repeats. Furthermore, I stratified by the target variable for increased accuracy.

As mentioned above, two recipes were used with the 6 model types, for a total of 12 models generated. The first recipe, which I will refer to as the *kitchen sink* recipe, includes every predictor variable and only the absolutely necessary feature engineering (ex. creating dummy variables for the categorical variables). This recipe was meant to provide a comparison for my more advanced recipe to see what impact my feature engineering has. This recipe was run on all 6 model types.

The second recipe, which I will call the *advanced* recipe, contains every predictor variable and more advanced feature engineering. Here again I created dummy variables for the categorical variables. Additionally, I included a step to transform the numerical variables that were not naturally normal. I chose this step (step_BoxCox) because it runs through all of the possible exponents for transformation and selects the best one for each variable. This maximized the affect of the transformations. Additionally, for the parametric models, I included interaction terms between variables that I found to be highly correlated during my EDA. Next, I removed variables that had near zero variance or zero variance so they did not disrupt the models' results. Finally, I centered and scaled all of the predictors. 


## Model Building & Selection Results

When building the models used in this project, I tuned all appropriate parameters based on balancing the dataset's unique needs and my computer's computational power. The parameters tuned were mtry, min_n and learn_rate for the boosted tree model. For the random forest model mtry and min_n were tuned. For the elastic net model the parameters tuned were penalty and mixture. Finally, for the k nearest neighbor model the neighbors parameter was tuned. For details of how each of the parameters were tuned and which parameters were best for each model, please see the appendix.

To choose which model and recipe combination is the best, I decided to use the RMSE metric to compare them. I chose this metric because of its harshness to outliers and the simplicity of being able to easily compare between models. As you can see below in @tbl-rmse, the models that use the *kitchen sink* recipe are labeled "KS" and the models that use the *advanced* recipe are labeled "Adv Rec". The model with the lowest and best RMSE value is the KS Random Forest model. 

```{r}
#| label: tbl-rmse
#| echo: false
#| tbl-cap: RMSE Values by Model


load(here("results/rmse_table.rda"))
knitr::kable(rmse_table)
```

The two random forest models were the best performing models, which demonstrates this model type is highly accurate for this kind of data. Next, the KNN and boosted tree models performed fairly similarly to each other and not much more poorly than the random forest models. These performed much better than the elastic net, linear, and null models. It's not surprising that one of the random forest models performed the best, because random forests tend to have high accuracy (at the cost of high computational need). This led me to hypothesize that either a random forest model or a boosted tree model would be the best. On the other hand, it was surprising to see that the *kitchen sink* recipe and the *advanced* recipe played almost no difference in the random forest model results. I expected the *advanced* recipe to produce better results, as it did in most of the other models. However, it also did not make a difference for the boosted tree model. Upon further reflection, I think this actually makes sense because the tree model recipe did not have an impact because there were no interaction terms in it, which likely caused an effect in the other models. Furthermore it is characteristic of tree models to be less influenced by a recipe and more just focused on the trees. Additionally, tree based models are more naturally able to adjust to irregular data, so the normalization and transformations also didn't play a big role.


## Final Model Analysis

The final model is going to be the *kitchen sink* random forest model because this model performed the best during testing. The assessment metrics for this model when fit on the testing data can be seen in @tbl-final-mets. As we can see, the RMSE here is even lower than it was when fit on the training data, which is a sign of a well fit model! Additionally, the MAE is fairly low at 4.327 and the R squared is fairly high at 0.76. Although this model is not perfect, it performs quite well considering it is modeling something so complex as music popularity.

```{r}
#| label: tbl-final-mets
#| tbl-cap: Metrics for Final Model
#| echo: false

load(here("results/final_mets.rda"))

knitr::kable(final_mets)
```


Furthermore, this model performs significantly better than the baseline models, which had RMSE values around 17. This shows there can be strong pay off of building more complex models, particularly when more accurate predictions are needed. We can see below in @fig-preds, the relationship between the final model's predictions and the actual popularity.

```{r}
#| label: fig-preds
#| fig-cap: Predicted vs actual popularity
#| echo: false

load(here("results/results_plot.rda"))
results_plot
```

As you can see in @fig-preds, most of the model's errors were by predicting albums would be more popular than they were. This is a very understandable error because the people who create music are likely also looking at these data and trying to make songs that fit with the data-driven explanation of popularity. However, it's very difficult to make your song be popular, even if you're following the data trends. I think it's interesting to point out how the highly popular albums were mostly very accurately predicted by the model. This shows that the model was able to parse out patterns and trends related to popular music.


## Conclusion

Overall, music is a very subjective art that realistically cannot (and maybe should not) be reduced to numbers on a screen. Despite this, I found it to be very interesting to explore the patterns in popular music. I think this research could move more into the psychology and cognitive science realm to dive into *why* people tend to like certain music and dislike others. Is this an output of collective cognition in a social world? Or are there universal patterns in what fundamentally sounds "good" and what does not?

## References

Agarwal, M., & Elemento. (2022). Music Albums Popularity Prediction. Kaggle. <https://www.kaggle.com/datasets/elemento/music-albums-popularity-prediction>

### Appendix: Tuning Parameters

The tuning parameters for the models were selected by balancing the data's need with the abilities of my computer. For your reference, I've compiled the tuning parameters, plots of their trials, and the parameters that were the best after fitting. This is displayed below in tables that include the parameters, their chosen values, the number of levels, and the best values (which were used for comparison between models). Each model was tuned in the same way, regardless of recipe, to ensure that the recipe was the only dependent variable in the comparison.

**Boosted Tree:**
```{r}
#| label: tbl-bt
#| tbl-cap: Boosted Tree Tuning Parameters
#| echo: false

knitr::kable(tibble(
  Parameter = c("mtry", "min_n", "learn_rate"),
  `Tuning Value` = c("1, 33", "2, 40", "-10, 5"),
  Levels = c(15, 15, 15),
  `KS Best Value` = c(28, 2, 0.44),
  `Adv Rec Best Value` = c(28, 2, 0.44)
))
```

```{r}
#| label: fig-bt-1
#| fig-cap: KS Boosted Tree
#| echo: false
load(here("results/bt_tuned.rda"))
load(here("results/bt_tuned_2.rda"))

autoplot(bt_tuned)

```

```{r}
#| label: fig-bt-2
#| fig-cap: Adv Rec Boosted Tree
#| echo: false

autoplot(bt_tuned_2)

```

**Elastic Net:**
```{r}
#| label: tbl-en
#| tbl-cap: Elastic Net Tuning Parameters
#| echo: false

knitr::kable(tibble(
  Parameter = c("penalty", "mixture"),
  `Tuning Value` = c("-10, 0", "0, 1"),
  Levels = c(20, 25),
  `KS Best Value` = c(0.0264, 0.05),
  `Adv Rec Best Value` = c(0.0078, 0.37)
))
```

```{r}
#| label: fig-en-1
#| fig-cap: KS Elastic Net
#| echo: false
load(here("results/elastic_tuned.rda"))
load(here("results/elastic_tuned_2.rda"))

autoplot(elastic_tuned)


```

```{r}
#| label: fig-en-2
#| fig-cap: Adv Rec Elastic Net
#| echo: false

autoplot(elastic_tuned_2)

```


**K Nearest Neighbor:**
```{r}
#| label: tbl-knn
#| tbl-cap: K Nearest Neighbor Tuning Parameters
#| echo: false

knitr::kable(tibble(
  Parameter = c("neighbors"),
  `Tuning Value` = c("1, 10"),
  Levels = c(9),
  `KS Best Value` = c(6),
  `Adv Rec Best Value` = c(5)
))
```

```{r}
#| label: fig-knn-1
#| fig-cap: KS K Nearest Neighbor
#| echo: false
load(here("results/knn_tuned.rda"))
load(here("results/knn_tuned_2.rda"))

autoplot(knn_tuned)


```

```{r}
#| label: fig-knn-2
#| fig-cap: Adv Rec K Nearest Neighbor
#| echo: false

autoplot(knn_tuned_2)

```

**Random Forest:**
```{r}
#| label: tbl-rf
#| tbl-cap: Random Forest Tuning Parameters
#| echo: false

knitr::kable(tibble(
  Parameter = c("min_n", "mtry"),
  `Tuning Value` = c("2, 40", "8, 15"),
  Levels = c(5, 5),
  `KS Best Value` = c(11, 2),
  `Adv Rec Best Value` = c(11, 2)
))
```

```{r}
#| label: fig-rf-1
#| fig-cap: KS Random Forest
#| echo: false
load(here("results/rf_tuned.rda"))
load(here("results/rf_tuned_2.rda"))

autoplot(rf_tuned)


```

```{r}
#| label: fig-rf-2
#| fig-cap: Adv Rec Random Forest
#| echo: false

autoplot(rf_tuned_2)

```

