---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Olivia Harbison"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[My Github Repo Link](https://github.com/stat301-2-2024-winter/final-project-2-oliviaharbison.git)

:::
```{r}
#| label: packages and data
#| echo: false

library(tidyverse)
library(tidymodels)
library(here)
set.seed(80)

```

## Data source

This dataset used data from a Spotify API that contains information about 160,000 songs. The information includes identifying variables (ex. song title) and numerical data (ex. danceability). There are 5 character variables, 1 date variable, and 39 numerical variables.^[This dataset can be found [here](https://www.kaggle.com/datasets/elemento/music-albums-popularity-prediction?select=train.csv) on kaggle.]


## Analysis Plan

For my data analysis, I'm going to split the data into training and testing groups. The training group will be 80% of the data, which is 15,594 observations. The testing group will be the remaining 20%, which is 3,902. This split will be stratified by the target variable (`popularity`). Furthermore, I will implement resampling via v-fold cross validation. For this, I will use 5 folds and 3 repeats because the dataset is quite large and my computer may not be able to handle much more. 

Once I've split and folded the data, I will generate recipes and models. I will use at least two recipes. One will be a "kitchen sink recipe," in which I use all of the possible predictor variables. The other will be more nuanced and based off of my data analysis and exploration. This recipe will include relevant interaction terms, necessary transformations, variable normalization, and more. 

I will then create 6 models of the following types: null (baseline), boosted tree, elastic net, k nearest neighbor, logistic, and random forest. I will run each on each of the two models (except for the null model which I will only run on the kitchen sink recipe). I will assess which of these models is the best fit based on the RMSE metric.


## Fitted Models

I've fitted two models to the resamples using a basic "kitchen sink" recipe. One of these models is a null model, and it is my baseline model for this project. The other model is a logistic model. As discussed above, I'm going to be using four more models and at least one more recipe as well. Below in @tbl-models, you can see each model's RMSE value.

```{r}
#| label: tbl-models
#| echo: false
#| tbl-cap: RMSE Values

load(here("results/rmse_tbl.rda"))
knitr::kable(rmse_tbl)

```



## Progress Update

As you can see, I've fitted two of my six models on the resamples with the kitchen sink recipe. I've also set up all of my scripts/folders for the rest of my project. My next step will be to do some more exploratory analysis on the data in order to create the second recipe. Then I will fit it to the rest of my models and analyze. I don't forsee any major issues or problems. 








